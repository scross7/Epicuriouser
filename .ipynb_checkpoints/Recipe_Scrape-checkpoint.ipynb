{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epicurious Recipe Webscrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guide to Web Scraping with Python Part 1: Requests and BeautifulSoup\n",
    "https://www.learndatasci.com/tutorials/ultimate-guide-web-scraping-w-python-requests-and-beautifulsoup/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from time import sleep\n",
    "import requests\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape recipe urls from a list of categories"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Scraping the urls and saving by search before scraping any recipes will allow us to target specific areas with broad categories by using keywords in the recipe url\n",
    "\n",
    "STEP 1\n",
    "- determine recipe category search terms (ex: healthy, beef, thanksgiving, etc.)\n",
    "\n",
    "STEP 2\n",
    "- generate a search url for each category, click_on \"SHOW: Recipes\"\n",
    "- inspect first page for list of recipes >> inspect for individual recipe link >> save link to list\n",
    "- click_on the next page and save links until ...\n",
    "\n",
    "STEP 3\n",
    "- loop through \"Scrape recipe page ... \" for each url in the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape recipe page, save HTML locally, read local HTML (DONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Do you want to save all of the scraped HTML files?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit specific recipe website >> add time.sleep(1) if site is loading slowly\n",
    "\n",
    "base = \"https://epicurious.com/recipes\"\n",
    "url = \"/food/views/overnight-porridge-congee-chao-andrea-nguyen-vietnamese-rice-soup/\"\n",
    "\n",
    "r = requests.get(base+url)\n",
    "\n",
    "# Save HTML locally\n",
    "def save_html(html, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        f.write(html)\n",
    "        \n",
    "save_html(r.content, 'recipe.html')\n",
    "\n",
    "# Read local HTML file\n",
    "def open_html(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return f.read()\n",
    "    \n",
    "    \n",
    "html = open_html('recipe.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soupify\n",
    "soup = BeautifulSoup(r.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape the recipe for general information (DONE)\n",
    "(not scraped: special equipment, preparation, menus, related content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "### do you want to scrape any of these sections above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe = dict()  \n",
    "recipe['title'] = soup.find('div', class_='title-source').h1.text                 # string\n",
    "recipe['author'] = soup.find(class_='contributor')['title']                       # string\n",
    "recipe['date'] = soup.find(class_=\"pub-date\").text.split()                        # string\n",
    "recipe['month'] = date[0]                                                         # string\n",
    "recipe['year'] = int(date[1])                                                     # int\n",
    "recipe['rating'] = float(soup.find(class_=\"user-interactions\").meta['content'])   # float\n",
    "recipe['reviews'] = int(soup.find(class_=\"reviews-count\").text)                   # int\n",
    "recipe['make_again'] = soup.find('div', class_=\"prepare-again-rating\").span.text  # int (float?)\n",
    "recipe['active_time'] = soup.find('dd', class_=\"active-time\").text                # string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingredients (DONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients = []\n",
    "\n",
    "lis = soup.find_all('li', class_=\"ingredient\")\n",
    "\n",
    "for li in lis:\n",
    "    newsoup = bs(str(li), 'html.parser')\n",
    "    ingredients.append(li.text)\n",
    "    \n",
    "recipe['ingredients'] = ingredients  # list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe['ingr_len'] = len(ingredients)  # int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nutritional Information"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- parse the data for all nutrition variables, write as a function that can be applied\n",
    "\n",
    "remove \"g\", \"(\" \"%\" \")\", then .split (' ')\n",
    "list = [12, 45]\n",
    "carb = {value_g: 12, percentage: 45}\n",
    "\n",
    "- parse the serving info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['28', '9']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carb = re.findall('\\d+', (soup.find('span', class_=\"nutri-data\", itemprop=\"carbohydrateContent\").text))\n",
    "carb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrition = dict()\n",
    "\n",
    "nutrition['cal'] = int(soup.find('span', class_=\"nutri-data\", itemprop=\"calories\").text)\n",
    "nutrition['carb'] = (soup.find('span', class_=\"nutri-data\", itemprop=\"carbohydrateContent\").text)\n",
    "nutrition['fat'] = soup.find('span', class_=\"nutri-data\", itemprop=\"fatContent\").text\n",
    "nutrition['protein'] = soup.find('span', class_=\"nutri-data\", itemprop=\"proteinContent\").text\n",
    "nutrition['sat_fat'] = soup.find('span', class_=\"nutri-data\", itemprop=\"saturatedFatContent\").text\n",
    "nutrition['sodium'] = soup.find('span', class_=\"nutri-data\", itemprop=\"sodiumContent\").text\n",
    "nutrition['polyunsat_fat'] = soup.findAll('span', class_=\"nutri-data\")[6].text # no attr = itemprop\n",
    "nutrition['fiber'] = soup.find('span', class_=\"nutri-data\", itemprop=\"fiberContent\").text\n",
    "nutrition['monounsat_fat'] = soup.findAll('span', class_=\"nutri-data\")[8].text # no attr = itemprop\n",
    "nutrition['cholesterol'] = soup.findAll('span', class_=\"nutri-data\")[9].text # no attr = itemprop\n",
    "nutrition['servings'] = soup.find(class_=\"per-serving\").text\n",
    "\n",
    "recipe['nutrition'] = nutrition # dict of dict [int, int]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "### What is the best way to save this data in a format that preserves the category and tag relationship?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Example HTML: \n",
    "<dl class=\"tags\">\n",
    "<a href=\"/ingredient/rice\"><dt itemprop=\"recipeCategory\">Rice</dt></a>\n",
    "\n",
    "Where: <a href=\"/category/tag ...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/cuisine/vietnamese'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tags = soup.find('dl', class_='tags').a['href'].split(\"/\")[1:]\n",
    "# tags = {\"category\" : category, \"tag\" : tag}\n",
    "\n",
    "tags = []\n",
    "\n",
    "dl_tags = soup.find_all('dl', class_='tags')\n",
    "\n",
    "for dl_tag in dl_tags:\n",
    "    newsoup = bs(str(dl_tag), 'html.parser')\n",
    "    tags.append(dl_tag.a['href'].split(\"/\")[1:])\n",
    "    \n",
    "# tags >> only outputs one tag group\n",
    "    \n",
    "# tag_len = len(tags)\n",
    "\n",
    "a = dl_tag.findAll('a')[1]['href']\n",
    "a\n",
    "\n",
    "# dl_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The whole shabang (DONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Super-Simple Overnight Porridge ',\n",
       " 'author': 'Andrea Nguyen',\n",
       " 'date': ['November', '2019'],\n",
       " 'month': 'November',\n",
       " 'year': 2019,\n",
       " 'rating': 0.0,\n",
       " 'reviews': 0,\n",
       " 'make_again': '0%',\n",
       " 'active_time': '30 minutes, plus overnight soaking',\n",
       " 'ingr_len': 7,\n",
       " 'nutrition': {'cal': 127,\n",
       "  'carb': ['28', 'g(9%)'],\n",
       "  'fat': '0 g(1%)',\n",
       "  'protein': '2 g(5%)',\n",
       "  'sat_fat': '0 g(0%)',\n",
       "  'sodium': '342 mg(14%)',\n",
       "  'polyunsat_fat': '0 g',\n",
       "  'fiber': '1 g(3%)',\n",
       "  'monounsat_fat': '0 g',\n",
       "  'cholesterol': '',\n",
       "  'servings': 'per serving (4 servings)'},\n",
       " 'ingredients': ['2 cups packed cooked white rice',\n",
       "  'About 5 cups chicken stock, vegetable stock, or store-bought chicken or vegetable broth',\n",
       "  '2 cups water, plus more as needed',\n",
       "  '3 thick slices unpeeled ginger, bruised',\n",
       "  '2 green onions, white parts kept whole, green parts cut into thin rings',\n",
       "  'About Â½ teaspoon fine sea salt',\n",
       "  'Recently ground black pepper (optional)']}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save recipe as a JSON (DONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('recipe.json', 'w') as f:\n",
    "    json.dump(all_recipes, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save multiple recipes to the category search JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Need to decide how to organize this information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_recipes = []\n",
    "all_recipes.append(recipe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conda [PythonData]",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
