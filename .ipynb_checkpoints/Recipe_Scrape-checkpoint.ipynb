{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epicurious Recipe Webscrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guide to Web Scraping with Python Part 1: Requests and BeautifulSoup\n",
    "https://www.learndatasci.com/tutorials/ultimate-guide-web-scraping-w-python-requests-and-beautifulsoup/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from time import sleep\n",
    "import requests\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape recipe urls from a list of categories"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Scraping the urls and saving by search before scraping any recipes will allow us to target specific areas with broad categories by using keywords in the recipe url\n",
    "\n",
    "STEP 1 (from sitemap) -> https://www.epicurious.com/services/sitemap\n",
    "- determine recipe category search terms (ex: healthy, beef, thanksgiving, etc.)\n",
    "\n",
    "STEP 2\n",
    "- generate a search url for each category, click_on \"SHOW: Recipes\"\n",
    "- inspect first page for list of recipes >> inspect for individual recipe link >> save link to list\n",
    "- click_on the next page and save links until ...\n",
    "\n",
    "STEP 3\n",
    "- loop through \"Scrape recipe page ... \" for each url in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.epicurious.com/services/sitemap/recipes/member/2019'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.epicurious.com/services/sitemap\n",
    "# https://www.epicurious.com/services/sitemap/recipes/member/2019\n",
    "member_url = \"https://www.epicurious.com/services/sitemap/recipes/member/\"\n",
    "year = \"2019\"\n",
    "member_year_url = member_url + year\n",
    "member_year_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the recipe urls from the first page\n",
    "r = requests.get(member_year_url)\n",
    "\n",
    "# Save HTML locally\n",
    "def save_html(html, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        f.write(html)\n",
    "        \n",
    "save_html(r.content, 'recipe_list.html')\n",
    "\n",
    "# Read local HTML file\n",
    "def open_html(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return f.read()\n",
    "    \n",
    "    \n",
    "html = open_html('recipe_list.html')\n",
    "soup = BeautifulSoup(r.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Omelet Benedict'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this only returns the first value ... \n",
    "# need to loop and save all values \n",
    "# then click to next page \n",
    "\n",
    "recipe_list_url = \"https://epicurious.com\"+soup.find('div', id=\"sitemapItems\").a['href']\n",
    "recipe_list_title = soup.find('div', id=\"sitemapItems\").a.text\n",
    "\n",
    "recipe_list_url # 'https://epicurious.com/recipes/member/views/omelet-benedict-5e08f2f054b6560009f82294'\n",
    "recipe_list_title # 'Omelet Benedict'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape recipe page, save HTML locally, read local HTML (DONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Do you want to save all of the scraped HTML files?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit specific recipe website >> add time.sleep(1) if site is loading slowly\n",
    "\n",
    "base = \"https://epicurious.com/recipes\"\n",
    "url = \"/food/views/overnight-porridge-congee-chao-andrea-nguyen-vietnamese-rice-soup/\"\n",
    "\n",
    "r = requests.get(base+url)\n",
    "\n",
    "# Save HTML locally\n",
    "def save_html(html, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        f.write(html)\n",
    "        \n",
    "save_html(r.content, 'recipe.html')\n",
    "\n",
    "# Read local HTML file\n",
    "def open_html(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return f.read()\n",
    "    \n",
    "    \n",
    "html = open_html('recipe.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soupify\n",
    "soup = BeautifulSoup(r.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape the recipe for general information (DONE)\n",
    "(not scraped: special equipment, preparation, menus, related content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### do you want to scrape any of these sections above?\n",
    "# write a functionfor each variable pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe = dict()  \n",
    "recipe['title'] = soup.find('div', class_='title-source').h1.text                 # string\n",
    "recipe['author'] = soup.find(class_='contributor')['title']                       # string\n",
    "date = soup.find(class_=\"pub-date\").text.split()                                  # string\n",
    "recipe['month'] = date[0]                                                         # string\n",
    "recipe['year'] = int(date[1])                                                     # int\n",
    "recipe['rating'] = float(soup.find(class_=\"user-interactions\").meta['content'])   # float\n",
    "recipe['reviews'] = int(soup.find(class_=\"reviews-count\").text)                   # int\n",
    "recipe['make_again'] = soup.find('div', class_=\"prepare-again-rating\").span.text  # int (float?)\n",
    "recipe['active_time'] = soup.find('dd', class_=\"active-time\").text                # string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Super-Simple Overnight Porridge ',\n",
       " 'author': 'Andrea Nguyen',\n",
       " 'month': 'November',\n",
       " 'year': 2019,\n",
       " 'rating': 4.0,\n",
       " 'reviews': 1,\n",
       " 'make_again': '100%',\n",
       " 'active_time': '30 minutes, plus overnight soaking'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingredients (DONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2 cups packed cooked white rice',\n",
       " 'About 5 cups chicken stock, vegetable stock, or store-bought chicken or vegetable broth',\n",
       " '2 cups water, plus more as needed',\n",
       " '3 thick slices unpeeled ginger, bruised',\n",
       " '2 green onions, white parts kept whole, green parts cut into thin rings',\n",
       " 'About Â½ teaspoon fine sea salt',\n",
       " 'Recently ground black pepper (optional)']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingredients = []\n",
    "\n",
    "lis = soup.find_all('li', class_=\"ingredient\")\n",
    "\n",
    "for li in lis:\n",
    "    newsoup = BeautifulSoup(str(li), 'html.parser')\n",
    "    ingredients.append(li.text)\n",
    "    \n",
    "recipe['ingredients'] = ingredients  # list of strings\n",
    "ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe['ingr_len'] = len(ingredients)  # int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nutritional Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cal': 127,\n",
       " 'carb': '28 g(9%)',\n",
       " 'fat': '0 g(1%)',\n",
       " 'protein': '2 g(5%)',\n",
       " 'sat_fat': '0 g(0%)',\n",
       " 'sodium': '342 mg(14%)',\n",
       " 'polyunsat_fat': '0 g',\n",
       " 'fiber': '1 g(3%)',\n",
       " 'monounsat_fat': '0 g',\n",
       " 'cholesterol': '',\n",
       " 'servings': 'per serving (4 servings)'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nutrition = dict()\n",
    "\n",
    "nutrition['cal'] = int(soup.find('span', class_=\"nutri-data\", itemprop=\"calories\").text)\n",
    "nutrition['carb'] = (soup.find('span', class_=\"nutri-data\", itemprop=\"carbohydrateContent\").text)\n",
    "nutrition['fat'] = soup.find('span', class_=\"nutri-data\", itemprop=\"fatContent\").text\n",
    "nutrition['protein'] = soup.find('span', class_=\"nutri-data\", itemprop=\"proteinContent\").text\n",
    "nutrition['sat_fat'] = soup.find('span', class_=\"nutri-data\", itemprop=\"saturatedFatContent\").text\n",
    "nutrition['sodium'] = soup.find('span', class_=\"nutri-data\", itemprop=\"sodiumContent\").text\n",
    "nutrition['polyunsat_fat'] = soup.findAll('span', class_=\"nutri-data\")[6].text # no attr = itemprop\n",
    "nutrition['fiber'] = soup.find('span', class_=\"nutri-data\", itemprop=\"fiberContent\").text\n",
    "nutrition['monounsat_fat'] = soup.findAll('span', class_=\"nutri-data\")[8].text # no attr = itemprop\n",
    "nutrition['cholesterol'] = soup.findAll('span', class_=\"nutri-data\")[9].text # no attr = itemprop\n",
    "nutrition['servings'] = soup.find(class_=\"per-serving\").text\n",
    "\n",
    "recipe['nutrition'] = nutrition # dict of dict [int, int]\n",
    "nutrition"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- parse the data for all nutrition variables, write as a function that can be applied\n",
    "\n",
    "remove \"g\", \"(\" \"%\" \")\", then .split (' ')\n",
    "list = [12, 45]\n",
    "carb = {value_g: 12, percentage: 45}\n",
    "\n",
    "- parse the serving info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['28', '9']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carb = re.findall('\\d+', (soup.find('span', class_=\"nutri-data\", itemprop=\"carbohydrateContent\").text))\n",
    "carb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### What is the best way to save this data in a format that preserves the category and tag relationship?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Example HTML: \n",
    "<dl class=\"tags\">\n",
    "<a href=\"/ingredient/rice\"><dt itemprop=\"recipeCategory\">Rice</dt></a>\n",
    "\n",
    "Where: <a href=\"/category/tag ...\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# so each cat would be it's own column (after scraping)\n",
    "\n",
    "elements = soup.findAll()\n",
    "\n",
    "def findElements(dataList,el):\n",
    "    temp=el.findChildren()\n",
    "    if(len(temp)==0):\n",
    "        print(el.get_text())\n",
    "\n",
    "tempResults=[findElements(dataList,el) for el in elements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = []\n",
    "tags = soup.find('dl', class_='tags').a['href'].split(\"/\")[1:]\n",
    "tags = {\"category\" : tags[0], \"tag\" : tags[1]}\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The whole shabang (DONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Super-Simple Overnight Porridge ',\n",
       " 'author': 'Andrea Nguyen',\n",
       " 'month': 'November',\n",
       " 'year': 2019,\n",
       " 'rating': 4.0,\n",
       " 'reviews': 1,\n",
       " 'make_again': '100%',\n",
       " 'active_time': '30 minutes, plus overnight soaking',\n",
       " 'ingredients': ['2 cups packed cooked white rice',\n",
       "  'About 5 cups chicken stock, vegetable stock, or store-bought chicken or vegetable broth',\n",
       "  '2 cups water, plus more as needed',\n",
       "  '3 thick slices unpeeled ginger, bruised',\n",
       "  '2 green onions, white parts kept whole, green parts cut into thin rings',\n",
       "  'About Â½ teaspoon fine sea salt',\n",
       "  'Recently ground black pepper (optional)'],\n",
       " 'ingr_len': 7,\n",
       " 'nutrition': {'cal': 127,\n",
       "  'carb': '28 g(9%)',\n",
       "  'fat': '0 g(1%)',\n",
       "  'protein': '2 g(5%)',\n",
       "  'sat_fat': '0 g(0%)',\n",
       "  'sodium': '342 mg(14%)',\n",
       "  'polyunsat_fat': '0 g',\n",
       "  'fiber': '1 g(3%)',\n",
       "  'monounsat_fat': '0 g',\n",
       "  'cholesterol': '',\n",
       "  'servings': 'per serving (4 servings)'}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save recipe as a JSON (DONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_recipes = []\n",
    "with open('recipe.json', 'w') as f:\n",
    "    json.dump(recipe, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save multiple recipes to the category search JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Need to decide how to organize this information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_recipes.append(recipe)\n",
    "with open('all_recipe.json', 'w') as f:\n",
    "    json.dump(all_recipes, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conda [PythonData]",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
