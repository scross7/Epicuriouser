{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epicurious Recipe Webscrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guide to Web Scraping with Python Part 1: Requests and BeautifulSoup\n",
    "https://www.learndatasci.com/tutorials/ultimate-guide-web-scraping-w-python-requests-and-beautifulsoup/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "from time import sleep\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape recipe urls from a list of categories"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Scraping the urls and saving by search before scraping any recipes will allow us to target specific areas with broad categories by using keywords in the recipe url\n",
    "\n",
    "- determine recipe category search terms (ex: healthy, beef, thanksgiving, etc.)\n",
    "- generate a search url for each category, click_on \"SHOW: Recipes\"\n",
    "- inspect first page for list of recipes >> inspect for individual recipe link >> save link to list\n",
    "- click_on the next page and save links until ...\n",
    "\n",
    "- loop through \"Scrape recipe page ... \" for each url in the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape recipe page, save HTML locally, read local HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit specific recipe website >> add time.sleep(1) if site is loading slowly\n",
    "\n",
    "base = \"https://epicurious.com/recipes\"\n",
    "url = \"/food/views/overnight-porridge-congee-chao-andrea-nguyen-vietnamese-rice-soup/\"\n",
    "\n",
    "r = requests.get(base+url)\n",
    "\n",
    "# Save HTML locally\n",
    "def save_html(html, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        f.write(html)\n",
    "        \n",
    "save_html(r.content, 'recipe.html')\n",
    "\n",
    "# Read local HTML file\n",
    "def open_html(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return f.read()\n",
    "    \n",
    "    \n",
    "html = open_html('recipe.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soupify\n",
    "soup = bs(r.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape the recipe for general information\n",
    "(not scraped: special equipment, preparation, menus, related content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### do you want to scrape any of these sections above? If we save individual raw html for each scrape we can add later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_recipes = [] # this will be a JSON of all recipes\n",
    "\n",
    "# for recipe in recipes:\n",
    "recipe = dict()  \n",
    "# d['name'] = row.select_one('.source-title').text.strip()\n",
    "# d['allsides_page'] = 'https://www.allsides.com' + row.select_one('.source-title a')['href']\n",
    "# d['bias'] = row.select_one('.views-field-field-bias-image a')['href'].split('/')[-1]\n",
    "# d['agree'] = int(row.select_one('.agree').text)\n",
    "# d['disagree'] = int(row.select_one('.disagree').text)\n",
    "# d['agree_ratio'] = d['agree'] / d['disagree']\n",
    "# d['agreeance_text'] = get_agreeance_text(d['agree_ratio'])\n",
    "    \n",
    "all_recipes.append(recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "title = soup.find('div', class_='title-source').h1.text\n",
    "author = soup.find(class_='contributor')['title']\n",
    "date = soup.find(class_=\"pub-date\").text.split()\n",
    "month = date[0]\n",
    "year = int(date[1])\n",
    "rating = float(soup.find(class_=\"user-interactions\").meta['content'])\n",
    "reviews = int(soup.find(class_=\"reviews-count\").text)\n",
    "make_again = soup.find('div', class_=\"prepare-again-rating\").span.text\n",
    "active_time = soup.find('dd', class_=\"active-time\").text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients = []\n",
    "\n",
    "lis = soup.find_all('li', class_=\"ingredient\")\n",
    "\n",
    "for li in lis:\n",
    "    newsoup = bs(str(li), 'html.parser')\n",
    "    ingredients.append(li.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingr_len = len(ingredients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nutritional Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Need to figure out how to parse and organize information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal = int(soup.find('span', class_=\"nutri-data\", itemprop=\"calories\").text)\n",
    "carb = soup.find('span', class_=\"nutri-data\", itemprop=\"carbohydrateContent\").text\n",
    "fat = soup.find('span', class_=\"nutri-data\", itemprop=\"fatContent\").text\n",
    "protein = soup.find('span', class_=\"nutri-data\", itemprop=\"proteinContent\").text\n",
    "sat_fat = soup.find('span', class_=\"nutri-data\", itemprop=\"saturatedFatContent\").text\n",
    "sodium = soup.find('span', class_=\"nutri-data\", itemprop=\"sodiumContent\").text\n",
    "polyunsat_fat = soup.findAll('span', class_=\"nutri-data\")[6].text # no attr = itemprop\n",
    "fiber = soup.find('span', class_=\"nutri-data\", itemprop=\"fiberContent\").text\n",
    "monounsat_fat = soup.findAll('span', class_=\"nutri-data\")[8].text # no attr = itemprop\n",
    "cholesterol = soup.findAll('span', class_=\"nutri-data\")[9].text # no attr = itemprop\n",
    "servings = soup.find(class_=\"per-serving\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cal': 127,\n",
       " 'carb': '28 g(9%)',\n",
       " 'fat': '0 g(1%)',\n",
       " 'protein': '2 g(5%)',\n",
       " 'sat_fat': '0 g(0%)',\n",
       " 'sodium': '342 mg(14%)',\n",
       " 'polyunsat_fat': '0 g',\n",
       " 'fiber': '1 g(3%)',\n",
       " 'monounsat_fat': '0 g',\n",
       " 'cholesterol': '',\n",
       " 'servings': 'per serving (4 servings)'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nutrition = {\n",
    "    \"cal\" : cal,                     # int\n",
    "    \"carb\" : carb,                   #\n",
    "    \"fat\" : fat,                     #\n",
    "    \"protein\" : protein,             #\n",
    "    \"sat_fat\" : sat_fat,             #\n",
    "    \"sodium\" : sodium,               #\n",
    "    \"polyunsat_fat\" : polyunsat_fat, #\n",
    "    \"fiber\" : fiber,                 #\n",
    "    \"monounsat_fat\" : monounsat_fat, # \n",
    "    \"cholesterol\" : cholesterol,     # \n",
    "    \"servings\" : servings            # \n",
    "}\n",
    "\n",
    "nutrition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### What is the best way to save this data in a format that preserves the category and tag relationship?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Example HTML: \n",
    "<dl class=\"tags\">\n",
    "<a href=\"/ingredient/rice\"><dt itemprop=\"recipeCategory\">Rice</dt></a>\n",
    "\n",
    "Where: <a href=\"/category/tag ...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<dl class=\"tags\"><a href=\"/ingredient/rice\"><dt itemprop=\"recipeCategory\">Rice</dt></a><a href=\"/cuisine/vietnamese\"><dt itemprop=\"recipeCuisine\">Vietnamese</dt></a><a href=\"/type/soup-stew\"><dt itemprop=\"recipeCategory\">Soup/Stew</dt></a><a href=\"/ingredient/ginger\"><dt itemprop=\"recipeCategory\">Ginger</dt></a><a href=\"/ingredient/green-onion-scallion\"><dt itemprop=\"recipeCategory\">Green Onion/Scallion</dt></a><a href=\"/special-consideration/healthy\"><dt itemprop=\"recipeCategory\">Healthy</dt></a><a href=\"/meal/breakfast\"><dt itemprop=\"recipeCategory\">Breakfast</dt></a><a href=\"/meal/lunch\"><dt itemprop=\"recipeCategory\">Lunch</dt></a></dl>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tags = soup.find('dl', class_='tags').a['href'].split(\"/\")[1:]\n",
    "# tags = {\"category\" : category, \"tag\" : tag}\n",
    "\n",
    "tags = []\n",
    "\n",
    "dl_tags = soup.find_all('dl', class_='tags')\n",
    "\n",
    "for dl_tag in dl_tags:\n",
    "    newsoup = bs(str(dl_tag), 'html.parser')\n",
    "    tags.append(dl_tag.a['href'].split(\"/\")[1:])\n",
    "    \n",
    "tag_len = len(tags)\n",
    "\n",
    "a = dl_tag.findAll('a')[0]['href']\n",
    "a\n",
    "\n",
    "dl_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The whole shabang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_data = {\n",
    "    \"title\" : title,             # string\n",
    "    \"author\" : author,           # string\n",
    "    \"month\" : month,             # string\n",
    "    \"year\" : year,               # int\n",
    "    \"rating\" : rating,           # float\n",
    "    \"reviews\" : reviews,         # int\n",
    "    \"make_again\" : make_again,   # int (not sure if this should be float?)\n",
    "    \"active_time\" : active_time, # string\n",
    "    \"ingr_len\" : ingr_len,       # int\n",
    "    \"ingredients\" : ingredients, # list of strings\n",
    "    \"nutrition\" : nutrition,     # dictionary\n",
    "    \"tags\": tags                 # list of dictionaries?\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data as a JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('recipe.json', 'w') as f:\n",
    "    json.dump(all_recipes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conda [PythonData]",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
