{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epicurious Yearly Recipe List\n",
    "\n",
    "Get the list of recipes and links to recipes from the sitemap by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from splinter import Browser\n",
    "import time\n",
    "from time import sleep\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Recipe URL List\n",
    "\n",
    "Only using recipes from the editors as member recipes don't seem to have nutritional information.\n",
    "\n",
    "This will pull a list of all the recipe titles and urls by year and put into a dataframe and save as csv so we can then loop through all recipes to pull details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define lists to store recipe URLs and Links. will zip these up togehter into dataframe at end\n",
    "sourcelist, yearlist, urllist, namelist = ([] for i in range(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define recipe urls and years to search\n",
    "root_url = \"https://www.epicurious.com\"\n",
    "sitemap_recipes_url = \"/services/sitemap/recipes\"\n",
    "source_urls = [\"/editorial/\"] # only used editorials since the members don't seem to have nutritional info\n",
    "years = [2019, 2018, 2017, 2016, 2015, 2014, 2013, 2012, 2011, 2010, 2009, \n",
    "         2008, 2007, 2006, 2005, 2004, 2003, 2002, 2001, 2000, 1999, 1998]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that parses the links and adds them to the lists\n",
    "def parse_recipe_links(source, year, links):\n",
    "    next_page_url = ''\n",
    "    counter = 0\n",
    "    for link in links:\n",
    "        counter += 1\n",
    "        try:\n",
    "            if sitemap_recipes_url in link['href']:\n",
    "                # the sitemap part of the url is in the link, this means it's not a recipe but a \"next page\"\n",
    "                if link.text == \"Next\":\n",
    "                    # we got a next page\n",
    "                    next_page_url = root_url + link['href']\n",
    "            else:\n",
    "                sourcelist.append(source)\n",
    "                yearlist.append(year)\n",
    "                urllist.append(link['href'].strip())\n",
    "                namelist.append(link.text.strip())\n",
    "        except:\n",
    "            print(f'Issue parsing recipe {counter}')\n",
    "            \n",
    "    return next_page_url;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start up the browser\n",
    "executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source_url in source_urls:\n",
    "    source = source_url.replace('/', '')\n",
    "    print(f'Beginning source: {source}')\n",
    "    for year in years:\n",
    "        print(f'Beginning year {year}')\n",
    "        url = root_url + sitemap_recipes_url + source_url + str(year)\n",
    "\n",
    "        while url != '':\n",
    "            print(url)\n",
    "            # visit url\n",
    "            browser.visit(url)\n",
    "            sleep(2)\n",
    "\n",
    "            # parse with beautiful soup\n",
    "            html = browser.html\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "            # find div of recipe links\n",
    "            try:\n",
    "                links = soup.find(id='sitemapItems').findAll('a')\n",
    "\n",
    "                # parse the recipe links. will return the next page or an empty string \n",
    "                url = parse_recipe_links(source, year, links)\n",
    "            except:\n",
    "                print(f'Error finding links for page {url}')\n",
    "                url = ''\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary from lists  \n",
    "dict_recipe_list = {'source': sourcelist, 'year': yearlist, 'title': namelist, 'url': urllist}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe from dictionary\n",
    "df = pd.DataFrame(dict_recipe_list) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export data to csv\n",
    "df.to_csv('resources/data/recipe_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conda [PythonData]",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
